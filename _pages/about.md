---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am a PhD candidate at Politecnico di Milano, working under the supervision of Prof. [Marcello Restelli](https://scholar.google.com/citations?user=xdgxRiEAAAAJ&hl=en) at the [RL^3 Group](https://rl.airlab.deib.polimi.it).

You can check out my (mostly up-to-date) CV [here](/files/Academic_CV.pdf){:target="_blank"}{:rel="noopener noreferrer"}, but for the freshest updates, my Scholar or BlueSky profiles might be more reliable. And if you're curious about anything, feel free to drop me an email, I’m always happy to chat!

My research is in **Reinforcement Learning** (RL), and I am especially interested in getting around some of the usual pain points, like relying on massive amounts of data, training everything from scratch, or needing centralized schemes when multiple agents are involved. Real-world applications don’t always play by those rules, unfortunately.

Lately, I have been focused on what we can do before the actual task is even defined, a field called **unsupervised RL**: things like **pre-training models** that make RL agents more general, more adaptable, and with more diverse behaviors. My goal is to develop methods that scale, generalize well, and are ready to tackle real-world messiness head-on.

To that end, I have dived into topics like **partial observability**, **multi-agency**, and decision-making under **general utility functions**. I have worked with **Siemens** to bring scalable multi-agent RL into industrial production scheduling, and more recently I have been collaborating with a London-based start-up, **Inephany**, on how to leverage RL for hyperparameter optimization in LLMs!


News et al.
---

(**2025**) 
- I successfully defended my [PhD Thesis](/files/thesis.pdf){:target="_blank"}{:rel="noopener noreferrer"}!
- New pre-print out! In [From Parameters to Behaviors: Unsupervised Compression of the Policy Space](https://arxiv.org/abs/2509.22566) we investigated how to compress behaviors without supervision in a scalable fashion and show how to do policy optimization in the (latent) behavior space! 
- After a couple of iterations, our work on unsupervised pre-training in MARL [Towards Principled Unsupervised Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2502.08365) was accepted at **NeurIPS 2025**! See you in San Diego/Mexico City/Copenhagen!
- I gave a talk at [Eugene Vinitsky](https://www.eugenevinitsky.com)'s amazing [Emerge Lab](https://emerge-lab.github.io) about new frontiers in unsupervised pre-training for Reinforcement Learning. You can find a simplified slide deck [here](/files/RG_EmergeLab.pdf){:target="_blank"}{:rel="noopener noreferrer"}.
- Our position paper on [Remembering the Markov Property in Cooperative MARL](https://arxiv.org/abs/2507.18333) got accepted at the [Finding the Frame](https://sites.google.com/view/findingtheframe/home) **workshop @ RLC 2025**! The awesome Kale-ab will be in Edmonton to present it, stop by! Also, it won the best poster award at [Deep Learning Indaba](https://deeplearningindaba.com/2025/)!
- I started a research collaboration with [Inephany](https://www.inephany.com) on the use of RL for large (language) models hyperparameter tuning. Excited to see what this will lead to!
- Good news for the explorers: the work with my amazing collaborators on [Enhancing Diversity in Parallel Agents: A Maximum State Entropy Exploration Story](https://arxiv.org/abs/2505.01336) was accepted at **ICML 2025**, and [Towards Unsupervised Multi-Agent Reinforcement Learning via Task-Agnostic Exploration](https://arxiv.org/abs/2502.08365) at the [EXAIT](https://exait-workshop.github.io) **workshop @ ICML 2025**. See you in Vancouver!
- I am presenting our recent work [Towards Principled Multi-Agent Task Agnostic Exploration](https://arxiv.org/abs/2502.08365) at the [UK Multi-Agent Systems Symposium](https://www.turing.ac.uk/events/uk-multi-agent-systems-symposium-2025-uk-mas). See you in London!
- Our work on [Scalable Multi-Agent Offline Reinforcement Learning and the Role of Information](https://arxiv.org/abs/2502.11260) was accepted at **RLDM 2025**! Down to chat about scalable solutions for Offline MARL in Dublin!

(**2024**) 

- I am visiting [David Abel](https://david-abel.github.io) and [Stefano V. Albrecht](https://agents-lab.org/stefano-albrecht/) at the [Autonomous Agents Research Group](https://agents-lab.org) in Edinburgh, U.K. for something like 6 months!
- I am giving a talk to the [RL Virtual Reading Group](https://agents.inf.ed.ac.uk/reading-group/) about advancing our understanding of learning purely explorative policies in POMDPs. You can find the recording on [You Tube](https://www.youtube.com/watch?v=hAxd6--b7TM).
- Grateful to be part of the Best Reviewer Award team at [ICML2024](https://icml.cc/virtual/2024/awards_detail).
- I am attending the [Machine Learning Summer School](https://groups.oist.jp/mlss) in Onna, Japan.
- Our paper [The Limits of Pure Exploration in POMDPs: When the Observation Entropy is Enough](https://rlj.cs.umass.edu/2024/papers/RLJ_RLC_2024_95.pdf) was accepted at the brand-new **RLC 2024** conference! I'll do a research tour between Vienna and Amherst to talk about unsupervised RL in POMDPs!
- Our paper [How to explore with belief: state entropy maximization in POMDPs](https://dl.acm.org/doi/10.5555/3692070.3694469) was accepted at **ICML 2024**. 

(**2023**) 

- I am attending the [Reinforcement Learning Summer School](https://rlsummerschool.com/) in Barcelona, Spain.
- The first paper of my Ph.D. [Distributional Policy Evaluation: a Maximum Entropy approach to Representation Learning](https://proceedings.neurips.cc/paper_files/paper/2023/hash/2a98af4fea6a24b73af7b588ca95f755-Abstract-Conference.html) was accepted at **NeurIPS 2023**! See you in New Orleans!

(**2022**)

- I am taking part to the organization of the [15th European Workshop on Reinforcement Learning](https://ewrl.wordpress.com/past-ewrl/ewrl15-2022/) in Milan, Italy.


